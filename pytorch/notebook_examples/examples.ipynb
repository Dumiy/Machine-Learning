{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from models.neuralnet import NeuralNet\n",
    "from models.convnet import ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1d', 'conv2d', 'conv3d', 'conv1dt', 'conv2dt', 'conv3dt'])\n",
      "torch.Size([1, 50, 49, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Types of convolution available\n",
    "\n",
    "from models.utils import return_conv_dict_keys\n",
    "print(return_conv_dict_keys())\n",
    "# How to create ConvNets\n",
    "model = ConvNet(3, [['conv2d', '15,3,2'],\n",
    "                    ['conv2dt', '35,3,2'],\n",
    "                    ['conv2dt', '55,3,2'],\n",
    "                    ['conv2d', '50,3,2']],\n",
    "                     [[nn.MaxPool2d((2,2)),nn.BatchNorm2d(15),nn.ReLU()],\n",
    "                      'skip', nn.ReLU(),\n",
    "                      nn.ReLU()])\n",
    "print(model(torch.randn(1,3,100,100)).shape)\n",
    "\n",
    "model = ConvNet(3, \n",
    "                    [['conv2d', 'out_channels=15,kernel_size=3,stride=2'],\n",
    "                    ['conv2dt', 'out_channels=30,kernel_size=3,stride=2'],\n",
    "                    ['conv2dt', 'out_channels=50,kernel_size=3,stride=2'],\n",
    "                    ['conv2d', 'out_channels=55,kernel_size=3,stride=2,padding=(2,2)']],\n",
    "                    [[nn.MaxPool2d((2,2)),nn.BatchNorm2d(15),nn.ReLU()],\n",
    "                     'skip', \n",
    "                     nn.ReLU(),\n",
    "                     nn.ReLU()])\n",
    "model(torch.randn(1,3,100,100)).shape == (1,55,51,51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet(100, [[15,True],\n",
    "                        [20,False], \n",
    "                        2],\n",
    "              [[torch.nn.ReLU(), torch.nn.BatchNorm1d(15)],\n",
    "               [torch.nn.ReLU(), torch.nn.BatchNorm1d(20)],\n",
    "               torch.nn.Sigmoid()])\n",
    "model(torch.randn(4,100)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for training ConvNet and NeuralNet to predict cat or dog  \n",
    "\n",
    "Using ImageClass for image and test accuracy function, any function can be custom written on any dataset or task  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import ImageClass,train,test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "dataset = os.listdir(\"train\") # dataset files containing all files\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog.6841.jpg',\n",
       " 'cat.7288.jpg',\n",
       " 'dog.2738.jpg',\n",
       " 'dog.10090.jpg',\n",
       " 'cat.6148.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [] # labeling the data for our case\n",
    "for x in dataset:\n",
    "    if 'cat' in x:\n",
    "        label_list.append(0)\n",
    "    else:\n",
    "        label_list.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split, can be changed to any train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(dataset,label_list,random_state=1,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating dataset with labels and spliting for train and testing we are gonna define preprocessing steps to do on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fd26f36730>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for image data\n",
    "preprocessing = transforms.Compose([transforms.ToPILImage(),\n",
    "                                    transforms.Resize((250,250)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.4,0.4,0.4),(0.1,0.1,0.1))\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Params for ImageClass \n",
    "# x_train = list of images\n",
    "# y_train = list of labels\n",
    "# preprocessing = function for preprocessing of data after reading\n",
    "# \"train\" -> folder of data\n",
    "\n",
    "train_loader = DataLoader(ImageClass(x_train,y_train,preprocessing,\"train\"),batch_size=16)\n",
    "test_loader = DataLoader(ImageClass(x_test,y_test,preprocessing,\"train\"),batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# First part for image extraction\n",
    "model = ConvNet(3, [['conv2d', 'out_channels=50,kernel_size=15,stride=2'],\n",
    "                    ['conv2d', 'out_channels=100,kernel_size=7,stride=2'],\n",
    "                    ['conv2d', 'out_channels=150,kernel_size=3,stride=1'],\n",
    "                    ['conv2d', 'out_channels=100,kernel_size=3,stride=1']],\n",
    "                    [[nn.MaxPool2d((2,3)),nn.BatchNorm2d(50),nn.ReLU()],\n",
    "                     [nn.MaxPool2d((2,2)),nn.BatchNorm2d(100),nn.ReLU()], \n",
    "                     [nn.MaxPool2d((2,2)),nn.BatchNorm2d(150),nn.ReLU()],\n",
    "                     [nn.ReLU(),nn.Flatten()]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    out_shape = model(torch.randn((1,3,250,250),requires_grad=False)).shape[1]\n",
    "    model.train()\n",
    "print(out_shape)\n",
    "# NeuralNet for feature classification extracted by the first model\n",
    "model2 = NeuralNet(out_shape, [528,500,9],\n",
    "              [torch.nn.ReLU(),torch.nn.ReLU(),\"skip\"])\n",
    "\n",
    "# We put ConvNet and Neural net in a list to pass to our custom model which combines both of them\n",
    "# Making it the final model\n",
    "models = [model,model2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.custom import CustomModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output step for neural net output\n",
    "def prediction_preprocess(x):\n",
    "    return torch.softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = CustomModel(models,prediction_preprocess).cuda() \n",
    "optim = torch.optim.Adam(final_model.parameters(),lr=1e-4) #optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim,step_size=12,gamma=0.1) #lr_scheduler \n",
    "loss = nn.CrossEntropyLoss() #classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function written for our own need\n",
    "def test(epoch,final_model,test_loader):\n",
    "    correct = 0 \n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_loader:\n",
    "            output = final_model(x.cuda())\n",
    "            correct+=(torch.argmax(output,dim=1) == y.cuda()).item()\n",
    "        print(\"Epoch {} - Accuracy - {:.2f}%\".format(epoch,(correct /len(test_loader))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Length 35 \tLoss: 2.193600\n",
      "Epoch 0 - Accuracy - 21.19%\n",
      "Train Epoch: 1 Length 35 \tLoss: 2.136061\n",
      "Epoch 1 - Accuracy - 33.90%\n",
      "Train Epoch: 2 Length 35 \tLoss: 1.839989\n",
      "Epoch 2 - Accuracy - 37.29%\n",
      "Train Epoch: 3 Length 35 \tLoss: 1.778563\n",
      "Epoch 3 - Accuracy - 44.92%\n",
      "Train Epoch: 4 Length 35 \tLoss: 2.069886\n",
      "Epoch 4 - Accuracy - 54.24%\n",
      "Train Epoch: 5 Length 35 \tLoss: 1.498630\n",
      "Epoch 5 - Accuracy - 56.36%\n",
      "Train Epoch: 6 Length 35 \tLoss: 1.712986\n",
      "Epoch 6 - Accuracy - 59.75%\n",
      "Train Epoch: 7 Length 35 \tLoss: 1.536920\n",
      "Epoch 7 - Accuracy - 60.59%\n",
      "Train Epoch: 8 Length 35 \tLoss: 1.398962\n",
      "Epoch 8 - Accuracy - 63.56%\n",
      "Train Epoch: 9 Length 35 \tLoss: 1.427579\n",
      "Epoch 9 - Accuracy - 61.86%\n",
      "Train Epoch: 10 Length 35 \tLoss: 1.550839\n",
      "Epoch 10 - Accuracy - 65.25%\n",
      "Train Epoch: 11 Length 35 \tLoss: 1.376087\n",
      "Epoch 11 - Accuracy - 65.25%\n",
      "Train Epoch: 12 Length 35 \tLoss: 1.378455\n",
      "Epoch 12 - Accuracy - 66.10%\n",
      "Train Epoch: 13 Length 35 \tLoss: 1.450982\n",
      "Epoch 13 - Accuracy - 66.95%\n",
      "Train Epoch: 14 Length 35 \tLoss: 1.377738\n",
      "Epoch 14 - Accuracy - 66.10%\n",
      "Train Epoch: 15 Length 35 \tLoss: 1.378418\n",
      "Epoch 15 - Accuracy - 66.10%\n",
      "Train Epoch: 16 Length 35 \tLoss: 1.373750\n",
      "Epoch 16 - Accuracy - 66.95%\n",
      "Train Epoch: 17 Length 35 \tLoss: 1.375106\n",
      "Epoch 17 - Accuracy - 67.80%\n",
      "Train Epoch: 18 Length 35 \tLoss: 1.385653\n",
      "Epoch 18 - Accuracy - 66.53%\n",
      "Train Epoch: 19 Length 35 \tLoss: 1.424500\n",
      "Epoch 19 - Accuracy - 66.95%\n",
      "Train Epoch: 20 Length 35 \tLoss: 1.558417\n",
      "Epoch 20 - Accuracy - 65.68%\n",
      "Train Epoch: 21 Length 35 \tLoss: 1.379072\n",
      "Epoch 21 - Accuracy - 65.68%\n",
      "Train Epoch: 22 Length 35 \tLoss: 1.374865\n",
      "Epoch 22 - Accuracy - 66.95%\n",
      "Train Epoch: 23 Length 35 \tLoss: 1.373677\n",
      "Epoch 23 - Accuracy - 64.41%\n",
      "Train Epoch: 24 Length 35 \tLoss: 1.377180\n",
      "Epoch 24 - Accuracy - 65.68%\n",
      "Train Epoch: 25 Length 35 \tLoss: 1.372881\n",
      "Epoch 25 - Accuracy - 65.25%\n",
      "Train Epoch: 26 Length 35 \tLoss: 1.375466\n",
      "Epoch 26 - Accuracy - 66.10%\n",
      "Train Epoch: 27 Length 35 \tLoss: 1.381625\n",
      "Epoch 27 - Accuracy - 66.53%\n",
      "Train Epoch: 28 Length 35 \tLoss: 1.375997\n",
      "Epoch 28 - Accuracy - 65.25%\n",
      "Train Epoch: 29 Length 35 \tLoss: 1.691637\n",
      "Epoch 29 - Accuracy - 65.25%\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(30):\n",
    "    train(epochs,final_model,optim,train_loader,loss)\n",
    "    scheduler.step()\n",
    "    test(epochs,final_model,test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
